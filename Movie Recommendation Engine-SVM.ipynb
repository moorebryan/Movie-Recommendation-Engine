{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are all modules used for this notebook.<br>\n",
    "Run the below box regardless of whether you are running one box or all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from multiprocessing import Pool,Process\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# The below suppresses all warnings in the notebook\n",
    "# Only leave this uncommented for display purposes\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the parameters which a user may want to change when running this program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ourCustID = 2630337 # ID of customer we want to make sure we have recommendations for (make sure not to exclude)\n",
    "train_frac = 0.50 # Percentage of data to use for training\n",
    "# Anyone who rated less movies than this will be excluded from consideration as a user to compare to\n",
    "min_movies_rated = 10\n",
    "# Min number of people rating a movie for it to be included\n",
    "min_raters_per_movie = 10\n",
    "# Fraction of customers to include for consideration in SVM\n",
    "cust_frac = 0.01\n",
    "# How many iterations to skip between displaying current error\n",
    "it_disp_rate = 10\n",
    "# Min difference allowed between two iterations before stopping the loop for performing SVD\n",
    "perc_change_min_lim = 0.01\n",
    "# Maximum number of iterations to go through for performing SVD\n",
    "iterations = 300\n",
    "# Number of features to include in the model\n",
    "K = 20\n",
    "# Learning rate for SVD implementation\n",
    "alpha = 0.002\n",
    "# Regularization parameter for SVD implementation\n",
    "beta = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This box creates the dataframe with all relevant information in the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning to format and combine the data...\n",
      "File with all user data has been created\n"
     ]
    }
   ],
   "source": [
    "global movie_data_file, a, b\n",
    "\n",
    "# Load movie dataset\n",
    "movie_data_file = pd.read_csv('movie_titles.csv', encoding=\"ISO-8859-1\", header=None, names=['Movie_Id', 'Year', 'title'])\n",
    "\n",
    "# Read IMDB  dataset for specific field names\n",
    "read_cols = ['genres', 'keywords', 'title', 'overview', 'popularity', 'vote_average', 'vote_count']\n",
    "IMDB_movie_list = pd.read_csv('tmdb_5000_movies.csv', skipinitialspace=True, usecols=read_cols)\n",
    "\n",
    "# Find union of two data frames and save this to a third list\n",
    "# Column_name should be common in both dataframes\n",
    "# 'how' represents type of intersection. In this case it is inner(INNER JOIN)\n",
    "output = pd.merge(IMDB_movie_list, movie_data_file, how=\"inner\",on=\"title\")\n",
    "\n",
    "final_output = output.drop_duplicates('title')  # Remove duplicate rows\n",
    "\n",
    "# Initialize dataframe\n",
    "combined_cd = pd.DataFrame(columns=['title', 'Cust_Id', 'Rating', 'Release-Date'])\n",
    "combined_cd = combined_cd.fillna(0)  # Fill missing values with zeros\n",
    "\n",
    "a = 'combined_data_'\n",
    "b = '.txt'\n",
    "\n",
    "# Create dataframe with requested columns\n",
    "def rename_by_movie(filenumber):\n",
    "    df = pd.read_csv(a + str(filenumber) + b, header=None, names=['Cust_Id', 'Rating'],\n",
    "                     usecols=[0, 1], dtype={'Rating': 'float'})\n",
    "    movie_index_indices = (df.loc[df['Rating'].isnull()]).index.values.tolist() # Indices with NaN\n",
    "    if len(movie_index_indices) <= 1: # Works if there is only one movie in the file\n",
    "        num1 = movie_index_indices[0]\n",
    "        movie_id = (df.loc[num1, :]['Cust_Id']).strip(\":\") # Strip off as this is always in 1st column\n",
    "        df.loc[num1:, 'Movie_ID'] = int(movie_id)\n",
    "    else:\n",
    "        i = 0\n",
    "        while (i + 1) < len(movie_index_indices): # Cycle through all movies in file\n",
    "            num1 = movie_index_indices[i] # Index for current movie\n",
    "            num2 = movie_index_indices[i + 1] # Following index for next movie\n",
    "            movie_id = (df.loc[num1, :]['Cust_Id']).strip(\":\") # Current movie ID\n",
    "            df.loc[num1:num2 - 1, 'Movie_ID'] = int(movie_id) # All indices for current movie ID\n",
    "            i += 1\n",
    "\n",
    "        movie_id = (df.loc[num2, :]['Cust_Id']).strip(\":\") # Accounts for last movie in current file\n",
    "        df.loc[num2:, 'Movie_ID'] = int( movie_id )\n",
    "\n",
    "    df = df[pd.notnull(df['Rating'])] # Remove rows with only movie ID given\n",
    "    df['Cust_Id'] = pd.to_numeric(df['Cust_Id'])  # Change customer ID to int data type\n",
    "    time.sleep(1) # Pause for one second upon completion (used for creating approximate progress bar)\n",
    "    return df\n",
    "\n",
    "print('Beginning to format and combine the data...')\n",
    "# Beginning combining all 4 pieces using parallel processing\n",
    "if __name__ == '__main__':\n",
    "    # Define the dataset to use for parallel processing\n",
    "    dataset = [1, 2, 3, 4]\n",
    "    agents = 4 # Number of cores to use\n",
    "    chunksize = 2\n",
    "\n",
    "    pool = Pool(processes=agents)  # Start 4 worker processes\n",
    "    \n",
    "    # Combine all four pieces into single result\n",
    "    df_comb = pd.concat(pool.map(rename_by_movie, dataset, chunksize))\n",
    "\n",
    "print('File with all user data has been created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run below to create the testing and training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out users below minimum threshold of movies rated...\n",
      "Removing excluded fraction of customers from being inlcuded in training or testing...\n",
      "Creating training and testing dataframes...\n",
      "Filtering out movies with less users who rated them than the minimum threshold...\n",
      "Pivoting the training dataframe...\n",
      "Converting pivoted dataframe to an array...\n",
      "Files with numpy array of users and movies and both dictionaries have created\n"
     ]
    }
   ],
   "source": [
    "print('Filtering out users below minimum threshold of movies rated...')\n",
    "# Below Section Removes users who have reated below a certain number of movies\n",
    "movie_count = pd.DataFrame()\n",
    "movie_count=df_comb.groupby(['Cust_Id']).size().reset_index(name='No_of_ratings')[['Cust_Id', 'No_of_ratings']]\n",
    "# Now remove users who rated below minimum threshold of movies\n",
    "mask = movie_count['No_of_ratings'] < min_movies_rated\n",
    "movie_count = movie_count.loc[~mask] # Remove users below that threshold from list\n",
    "\n",
    "# Now get list of unique users remaining\n",
    "unique_users=movie_count.loc[:,'Cust_Id'].unique().tolist()\n",
    "del movie_count # Delete for memory management\n",
    "\n",
    "#Keep only those users in df_comb\n",
    "mask = df_comb['Cust_Id'].isin(unique_users) # Find locations of remaining users in main dataframe\n",
    "df_comb = df_comb.loc[mask] # Keep only those users in the dataframe\n",
    "\n",
    "# Save the dataframe for the user we want to make sure is included\n",
    "df_chosen_cust = df_comb.loc[df_comb['Cust_Id'] == ourCustID]\n",
    "\n",
    "print('Removing excluded fraction of customers from being inlcuded in training or testing...')\n",
    "# Minimize number of customers from dataset to be included in training or testing dataframes\n",
    "unique_users=df_comb.loc[:,'Cust_Id'].unique().tolist()\n",
    "num_cust_to_sample = int(round(len(unique_users) * cust_frac))\n",
    "cust_IDs_to_Keep = random.sample(unique_users, num_cust_to_sample)\n",
    "mask = df_comb['Cust_Id'].isin(cust_IDs_to_Keep)\n",
    "df_comb = df_comb.loc[mask]  # No use looking at movies not relating to users left in dataset\n",
    "\n",
    "# Make sure our chosen customer has not been excluded\n",
    "incl_users = df_comb.loc[:,'Cust_Id'].unique().tolist()\n",
    "if ourCustID not in incl_users:\n",
    "    df_comb.append(df_chosen_cust)\n",
    "\n",
    "print('Creating training and testing dataframes...')\n",
    "# Now take desired fraction of data for use in training set\n",
    "df_train=df_comb.sample(frac=train_frac)\n",
    "\n",
    "df_test=df_comb.drop(df_train.index)\n",
    "del df_comb # Delete for memory management\n",
    "\n",
    "# Create a dictionary containing correlation between row number and Customer ID\n",
    "unique_users=df_train.loc[:,'Cust_Id'].tolist()\n",
    "user_dict=[(row_index, user_id) for row_index, user_id in enumerate(unique_users)]\n",
    "\n",
    "print('Filtering out movies with less users who rated them than the minimum threshold...')\n",
    "# Below Section Removes movies below a certain threshold of users who have rated them\n",
    "user_count = pd.DataFrame()\n",
    "user_count=df_train.groupby(['Movie_ID']).size().reset_index(name='No_of_raters')[['Movie_ID', 'No_of_raters']]\n",
    "\n",
    "# Now remove movies with less than the minimum of threshold of people who rated them\n",
    "mask = user_count['No_of_raters'] < min_raters_per_movie\n",
    "user_count = user_count.loc[~mask]\n",
    "\n",
    "# Get list of unique movies remaining\n",
    "unique_movies=user_count.loc[:,'Movie_ID'].unique().tolist()\n",
    "del user_count # Delete for memory management\n",
    "\n",
    "# Keep only those movies in df_comb\n",
    "mask = df_train['Movie_ID'].isin(unique_movies)\n",
    "df_train = df_train.loc[mask]\n",
    "\n",
    "# Create a dictionary containing correlation between column number and Movie ID\n",
    "unique_movies=df_train.loc[:,'Movie_ID'].tolist()\n",
    "movie_dict = [(col_index, movie_id) for col_index, movie_id in enumerate(unique_movies)]\n",
    "\n",
    "# Format dataframe with row indices as Users and column indices as Movies\n",
    "print('Pivoting the training dataframe...')\n",
    "df_train=df_train.pivot(index = 'Cust_Id', columns ='Movie_ID', values = 'Rating').fillna(0)\n",
    "\n",
    "print('Converting pivoted dataframe to an array...')\n",
    "# convert to numpy array\n",
    "train_array = df_train.values\n",
    "del df_train # Delete for memory management\n",
    "print('Files with numpy array of users and movies and both dictionaries have created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This box creates predictions for all users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training iterations...\n",
      "At iteration: 10 of max 300 the error is 635.69\n",
      "\tIt has been 3.84 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.22\n",
      "\tApproximately 111.71 minutes remain until iteration max limit is reached\n",
      "At iteration: 20 of max 300 the error is 627.06\n",
      "\tIt has been 7.6 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.11\n",
      "\tApproximately 106.83 minutes remain until iteration max limit is reached\n",
      "At iteration: 30 of max 300 the error is 616.92\n",
      "\tIt has been 11.13 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.24\n",
      "\tApproximately 100.53 minutes remain until iteration max limit is reached\n",
      "At iteration: 40 of max 300 the error is 595.90\n",
      "\tIt has been 14.61 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.4\n",
      "\tApproximately 95.35 minutes remain until iteration max limit is reached\n",
      "At iteration: 50 of max 300 the error is 572.17\n",
      "\tIt has been 18.16 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.42\n",
      "\tApproximately 91.14 minutes remain until iteration max limit is reached\n",
      "At iteration: 60 of max 300 the error is 548.85\n",
      "\tIt has been 21.73 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.41\n",
      "\tApproximately 87.28 minutes remain until iteration max limit is reached\n",
      "At iteration: 70 of max 300 the error is 528.57\n",
      "\tIt has been 25.24 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.35\n",
      "\tApproximately 83.3 minutes remain until iteration max limit is reached\n",
      "At iteration: 80 of max 300 the error is 512.19\n",
      "\tIt has been 28.79 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.29\n",
      "\tApproximately 79.54 minutes remain until iteration max limit is reached\n",
      "At iteration: 90 of max 300 the error is 499.06\n",
      "\tIt has been 32.33 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.24\n",
      "\tApproximately 75.8 minutes remain until iteration max limit is reached\n",
      "At iteration: 100 of max 300 the error is 488.37\n",
      "\tIt has been 35.87 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.2\n",
      "\tApproximately 72.1 minutes remain until iteration max limit is reached\n",
      "At iteration: 110 of max 300 the error is 479.49\n",
      "\tIt has been 39.41 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.17\n",
      "\tApproximately 68.43 minutes remain until iteration max limit is reached\n",
      "At iteration: 120 of max 300 the error is 471.99\n",
      "\tIt has been 42.94 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.15\n",
      "\tApproximately 64.77 minutes remain until iteration max limit is reached\n",
      "At iteration: 130 of max 300 the error is 465.59\n",
      "\tIt has been 46.47 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.13\n",
      "\tApproximately 61.13 minutes remain until iteration max limit is reached\n",
      "At iteration: 140 of max 300 the error is 460.05\n",
      "\tIt has been 50.0 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.11\n",
      "\tApproximately 57.5 minutes remain until iteration max limit is reached\n",
      "At iteration: 150 of max 300 the error is 455.22\n",
      "\tIt has been 53.53 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.1\n",
      "\tApproximately 53.88 minutes remain until iteration max limit is reached\n",
      "At iteration: 160 of max 300 the error is 450.98\n",
      "\tIt has been 57.04 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.09\n",
      "\tApproximately 50.27 minutes remain until iteration max limit is reached\n",
      "At iteration: 170 of max 300 the error is 447.23\n",
      "\tIt has been 60.59 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.08\n",
      "\tApproximately 46.69 minutes remain until iteration max limit is reached\n",
      "At iteration: 180 of max 300 the error is 443.89\n",
      "\tIt has been 64.12 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.07\n",
      "\tApproximately 43.1 minutes remain until iteration max limit is reached\n",
      "At iteration: 190 of max 300 the error is 440.90\n",
      "\tIt has been 67.67 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.07\n",
      "\tApproximately 39.54 minutes remain until iteration max limit is reached\n",
      "At iteration: 200 of max 300 the error is 438.20\n",
      "\tIt has been 71.19 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.06\n",
      "\tApproximately 35.95 minutes remain until iteration max limit is reached\n",
      "At iteration: 210 of max 300 the error is 435.78\n",
      "\tIt has been 74.74 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.05\n",
      "\tApproximately 32.39 minutes remain until iteration max limit is reached\n",
      "At iteration: 220 of max 300 the error is 433.57\n",
      "\tIt has been 78.25 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.05\n",
      "\tApproximately 28.81 minutes remain until iteration max limit is reached\n",
      "At iteration: 230 of max 300 the error is 431.56\n",
      "\tIt has been 81.8 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.04\n",
      "\tApproximately 25.25 minutes remain until iteration max limit is reached\n",
      "At iteration: 240 of max 300 the error is 429.73\n",
      "\tIt has been 85.32 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.04\n",
      "\tApproximately 21.69 minutes remain until iteration max limit is reached\n",
      "At iteration: 250 of max 300 the error is 428.04\n",
      "\tIt has been 88.84 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.04\n",
      "\tApproximately 18.12 minutes remain until iteration max limit is reached\n",
      "At iteration: 260 of max 300 the error is 426.49\n",
      "\tIt has been 92.36 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.03\n",
      "\tApproximately 14.57 minutes remain until iteration max limit is reached\n",
      "At iteration: 270 of max 300 the error is 425.05\n",
      "\tIt has been 95.83 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.03\n",
      "\tApproximately 11.0 minutes remain until iteration max limit is reached\n",
      "At iteration: 280 of max 300 the error is 423.72\n",
      "\tIt has been 99.3 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.03\n",
      "\tApproximately 7.45 minutes remain until iteration max limit is reached\n",
      "At iteration: 290 of max 300 the error is 422.48\n",
      "\tIt has been 102.74 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.03\n",
      "\tApproximately 3.9 minutes remain until iteration max limit is reached\n",
      "At iteration: 300 of max 300 the error is 421.33\n",
      "\tIt has been 106.18 minutes since the function started\n",
      "\tThe percent difference stop condition is set to 0.01 and currently it is at 0.03\n",
      "\tApproximately 0.35 minutes remain until iteration max limit is reached\n",
      "Training is complete...\n",
      "Creating full prediction matrix...\n",
      "All requested ratings have been generated\n"
     ]
    }
   ],
   "source": [
    "# Create class for performing the matrix factorization\n",
    "class Matrix_Factoring():\n",
    "\n",
    "    # Initializing the user-movie rating matrix,no of hidden features,bias alpha and beta\n",
    "    def __init__(self, R, K, alpha, beta, iterations, perc_change_min_lim):\n",
    "        self.R = R\n",
    "        self.num_unique_users, self.num_unique_movies = self.R.shape\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "        \n",
    "        # List of training samples (Start with none and build)\n",
    "        self.dicts_row = dict()\n",
    "        self.dicts_column = dict()\n",
    "        \n",
    "        # Intialize the P and Q matrix (Only done once) (Over k to normalize for below summations)\n",
    "        self.P = np.random.normal(scale=1. / self.K, size=(self.num_unique_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1. / self.K, size=(self.num_unique_movies, self.K))\n",
    "        \n",
    "        # Initialize the associated bias terms (Start with bias for users and movies set to zero)\n",
    "        self.b_P = np.zeros(self.num_unique_users)\n",
    "        self.c_Q = np.zeros(self.num_unique_movies)\n",
    "        mask = (self.R != 0)  # Find entries in the true rating matrix where we have actual ratings\n",
    "        self.mu = np.mean(self.R[mask])  # Overall average rating across all users\n",
    "        self.perc_change_min_lim = perc_change_min_lim\n",
    "\n",
    "    # Main definition in this class (iteratively trains weighting for all data points)\n",
    "    def train(self):\n",
    "        temp = []\n",
    "        temp1 = []\n",
    "        t0 = time.time()\n",
    "        for i in range(self.num_unique_users):\n",
    "            temp = self.R[i,:].tolist()  # Holds list of all ratings for user i\n",
    "            temp1 = [(num, entryvalue) for num, entryvalue in enumerate(temp) if entryvalue > 0]\n",
    "            self.dicts_row[i] = temp1  # Store dict for user i\n",
    "\n",
    "        training_process = []\n",
    "        count = 0\n",
    "        perc_change = 100  # Initialize so following loop occurs\n",
    "        while perc_change > self.perc_change_min_lim: # This is the iterative gradient descent\n",
    "            t1 = time.time()\n",
    "            # Make list of keys and list of values and zip them together, will preserve order\n",
    "            a = list(self.dicts_row.keys())  # List of all row indices corresponding to users\n",
    "            b = list(self.dicts_row.values())  # List of all (movie,ratings) combinations corresponding to all rated movies for each user\n",
    "            # Below is a list within a list\n",
    "            combined = list(zip(a, b))  # Creates list, one for each user, holding (movie_index,rating) for each rated movie\n",
    "            random.shuffle(combined)  # Randomizing order of the users to be considered\n",
    "\n",
    "            # Creates new dictionary, using the reordered users\n",
    "            self.dicts_row = {x[0]: x[1] for x in combined}\n",
    "\n",
    "            # Update weights for P,Q,b_P,c_Q using stochastic gradient descent (for all users and all movies)\n",
    "            self.sgd()\n",
    "\n",
    "            # Calculate overall error for the predicted matrix\n",
    "            error = self.mean_squared_error()\n",
    "            training_process.append((count, error))  # Holds list of iteration, error\n",
    "\n",
    "            if count > 0:  # Only makes sense after at least one iteration has occured\n",
    "                perc_change = abs(100 * (training_process[count-1][1] - training_process[count][1]) / training_process[count][1])\n",
    "            if (count-1) > self.iterations:  # Artificially end loop if we reach set number of iterations\n",
    "                perc_change = 0\n",
    "\n",
    "            # Display progress at provided frequency\n",
    "            if (count + 1) % it_disp_rate == 0:\n",
    "                t2 = time.time()\n",
    "                time_elapsed = (t2-t0)/60\n",
    "                est_time_per_iteration = time_elapsed / float(count+1)\n",
    "                est_time_remaining = est_time_per_iteration * (self.iterations - count)\n",
    "                print(\"At iteration: %d of max %d the error is %.2f\" % (count + 1, self.iterations, error))\n",
    "                print('\\t' + 'It has been ' + str(round(time_elapsed, 2)) + ' minutes since the function started')\n",
    "                print('\\t' + 'The percent difference stop condition is set to ' + str(self.perc_change_min_lim) + ' and currently it is at ' + str(round(perc_change,2)))\n",
    "                print('\\t' + 'Approximately ' + str(round(est_time_remaining, 2)) + ' minutes remain until iteration max limit is reached')\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        print('Training is complete...')\n",
    "        return training_process  # So we can plot error progress for chosen k\n",
    "\n",
    "    # Predicts rating specific to user i and movie j\n",
    "    def get_rating(self, row, column):\n",
    "        prediction = self.mu + self.b_P[row] + self.c_Q[column] + self.P[row, :].dot(self.Q[column, :].T)\n",
    "        return prediction #Predicted rating for given user and movie\n",
    "\n",
    "    # Updates weights for P,Q,b_P,c_Q by stochastic gradient descent,mu stays the same\n",
    "    def sgd(self):\n",
    "        for key, value in self.dicts_row.items():\n",
    "            row = int(key)  # Represents current customer we are looking at\n",
    "            list_of_movies=np.asarray(value)  # List of rated (movie_index, rating) for that customer\n",
    "            \n",
    "            for l in range(len(value)):  # Go through every rated movie for that customer\n",
    "                column = int(list_of_movies[l, 0])  # Holds movie index\n",
    "                rating = list_of_movies[l, 1]  # Holds rating associated with that movie\n",
    "                predicted_rating = self.get_rating(row, column)  # Holds predicted rating for that movie\n",
    "                e = rating-predicted_rating  # Difference between actual rating and predicted rating\n",
    "                \n",
    "                # Update all matrices for this particular user, movie combination\n",
    "                self.b_P[row] += self.alpha * (e-self.beta * self.b_P[row])\n",
    "                self.c_Q[column] += self.alpha * (e-self.beta * self.c_Q[column])\n",
    "                self.P[row, :] += self.alpha*(e * self.Q[column, :] - self.beta * self.P[row, :])\n",
    "                self.Q[column, :] += self.alpha * (e * self.P[row, :] - self.beta * self.Q[column, :])\n",
    "\n",
    "    # Calculates overall error for predicted matrix\n",
    "    def mean_squared_error(self):\n",
    "        l_row, l_column = self.R.nonzero()  # List of row and column index for non zero elements in R.\n",
    "        predicted = self.full_matrix()  # Holds full prediction matrix with ratings for all customers and movies\n",
    "        error = 0  # Initialize for iteration\n",
    "        \n",
    "        for row, column in zip(l_row, l_column):  # Go through each user,movie combination in full matrix\n",
    "            error += pow(self.R[row, column]-predicted[row, column], 2)\n",
    "        return np.sqrt(error)  # Holds mean squared error\n",
    "\n",
    "    # Creates and returns the full prediction matrix\n",
    "    def full_matrix(self):\n",
    "        return self.mu + self.b_P.reshape(-1, 1) + self.c_Q[np.newaxis:, ] + self.P.dot(self.Q.T)\n",
    "\n",
    "mf = Matrix_Factoring(train_array, K, alpha, beta, iterations, perc_change_min_lim)  # Call and initialize class\n",
    "print('Beginning training iterations...')\n",
    "training_errors = mf.train()  # Begin training using the function in the class\n",
    "\n",
    "print('Creating full prediction matrix...')\n",
    "prediction_matrix = mf.full_matrix()  # Save the prediction matrix after going through all iterations\n",
    "\n",
    "user_bias = mf.b_P\n",
    "movie_bias = mf.c_Q\n",
    "user_feature_mat = mf.P\n",
    "movie_feature_mat = mf.Q\n",
    "\n",
    "print('All requested ratings have been generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next portion, not written yet, will use test set to see how general accurate the predictions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
